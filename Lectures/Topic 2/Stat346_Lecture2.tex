\documentclass[compress,mathserif]{beamer}

\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{amsxtra}
\usepackage{hyperref}
\usepackage{tensor}
\usepackage{dsfont}

\mode<presentation> {
  \usetheme{default}
  \useoutertheme{infolines}
  \setbeamercovered{transparent}
  \beamertemplateballitem
}

\title[Claims Frequency Models]{Claims Frequency Distribution Models}
\author{Chapter 6}
\institute[Stat 346]{Stat 346 - Short-term Actuarial Math}
\date{}
\subject{Stat 477}
\begin{document}

\begin{frame}
 \titlepage
\end{frame}

\section{Introduction}
\frame{\frametitle{Introduction}
\begin{itemize}
\item Here we introduce a large class of counting distributions, which are discrete distributions with support consisting of non-negative integers.
\smallskip
\item Generally used for modeling number of events, but in an insurance context, the number of claims within a certain period, e.g. one year.
\smallskip
\item We call these \alert{claims frequency} models.
\smallskip
\item Let $N$ denote the number of events (or claims). Its probability mass function (pmf), $p_k = \text{Pr}(N=k)$, for $k=0,1,2,\ldots$, gives the probability that exactly $k$ events (or claims) occur.
\end{itemize}
}

%\subsection{probability generating function}
%\frame{\frametitle{The probability generating function}
%\begin{itemize}
%\item Recall the pgf of $N$: $P_N(z) = \text{E}(z^N) = \sum_{k=0}^\infty p_k z^k$.
%\item Just like the mgf, pgf also generates moments:
%\begin{equation*}
%P'_N(1) = \text{E}(N) \ \ \text{and} \ \ P''_N(1) = \text{E}[N(N-1)].
%\end{equation*}
%\item More importantly, it generates probabilities:
%\begin{eqnarray*}
%P^{(m)}_N(z) = \text{E} \biggl( \frac{d^m}{dz^m} z^N \biggr) &=& \text{E}[N(N-1)\cdots(N-m+1)z^{N-m}] \\
%&=& \sum_{k=m}^{\infty} k(k-1)\cdots(k-m+1)z^{k-m}p_k
%\end{eqnarray*}
%\item Thus, we see that
%\begin{equation*}
%P^{(m)}_N(0) = m!\, p_m \ \ \text{or} \ \ p_m = \frac{1}{m!}P^{(m)}(0).
%\end{equation*}
%\end{itemize}
%}

\section{Some discrete distributions}
\frame{\frametitle{Some familiar discrete distributions}
Some of the most commonly used distributions for number of claims:
\smallskip
\begin{itemize}
\item Binomial (with Bernoulli as special case)
\smallskip
\item Poisson
\smallskip
\item Geometric
\smallskip
\item Negative Binomial
\smallskip
\item The $(a,b,0)$ class
\smallskip
\item The $(a,b,1)$ class
\end{itemize}
}

\section{Bernoulli random variables}
\frame{\frametitle{Bernoulli random variables}
\begin{itemize}
\item $N$ is \alert{Bernoulli} if it takes only one of two possible outcomes:
\begin{equation*}
N = \left\{
\begin{array}{ll}
1, & \text{if a claim occurs} \\
0, & \text{otherwise}
\end{array}
\right. .
\end{equation*}
\item $q$ is the standard symbol for the probability of a claim, i.e. $\text{Pr}(N=1) = q$.
\smallskip
\item We write $N \sim \text{Bernoulli}(q)$.
\smallskip
\item Mean $\text{E}(N) = q$ and variance $\text{Var}(N) = q(1-q)$
\smallskip
%\item Probability generating function:
%\begin{equation*}
%P_N(z) = qz + (1-q)
%\end{equation*}
\end{itemize}
}

\section{Binomial random variables}
\frame{\frametitle{Binomial random variables}
\begin{itemize}
\item We write $N \sim \text{Binomial}(m,q)$ if $N$ has a \alert{Binomial} distribution with pmf:
\begin{equation*}
p_k = \text{Pr}(N=k) = \binom{m}{k} q^k (1-q)^{m-k} = \frac{m!}{k!(m-k)!} q^k (1-q)^{m-k},
\end{equation*}
for $k=0,\ldots,m$.
\item Binomial r.v. is also the sum of independent Bernoulli's with $N = \sum_{k=1}^{m} N_k$ where each $N_k \sim \text{Bernoulli(q)}$.
\smallskip
\item Mean $\text{E}(N) = mq$ and variance $\text{Var}(N) = mq(1-q)$
\smallskip
%\item Probability generating function:
%\begin{equation*}
%P_N(z) = [qz + (1-q)]^m
%\end{equation*}
\end{itemize}
}

\section{Poisson random variables}
\frame{\frametitle{Poisson random variables}
\begin{itemize}
\item $N \sim \text{\alert{Poisson}}(\lambda)$ if pmf is
\begin{equation*}
p_k = P(X=k) = e^{-\lambda} \frac{\lambda^k}{k!}, \ \ \text{for} \ k=0,1,2,\ldots
\end{equation*}
\item Mean and variance are equal: $\text{E}(N) = \text{Var}(N) = \lambda$
\smallskip
%\item Probability generating function of a Poisson:
%\begin{equation*}
%P_N(z) = e^{\lambda(z -1)}.
%\end{equation*}
\item Sums of independent Poissons: If $N_1,\ldots,N_n$ be $n$ independent Poisson variables with parameters $\lambda_1,\ldots,\lambda_n$, then the sum
\begin{equation*}
N = N_1 + \cdots + N_n
\end{equation*}
has a Poisson distribution with parameter $\lambda = \lambda_1 + \cdots + \lambda_n$.
\end{itemize}
}

%\subsection{decomposition}
%\frame{\frametitle{Decomposition property of the Poisson}
%\begin{itemize}
%\item Suppose a certain number, $N$, of events will occur and $N \sim \text{Poisson}(\lambda)$.
%\smallskip
%\item Suppose further that each event is either a Type 1 event with probability $p$ or a Type 2 event with probability $1-p$.
%\smallskip
%\item Let $N_1$ and $N_2$ be the number of Types 1 and 2 events, respectively, so that $N=N_1+N_2$.
%\smallskip
%\item Result: $N_1$ and $N_2$ are independent Poisson random variables with respective means
%\begin{equation*}
%\text{E}(N_1) = \lambda p \ \ \text{and} \ \ \text{E}(N_2) = \lambda (1-p).
%\end{equation*}
%%\item Proof to be provided in class.
%%\smallskip
%\item This result can be extended to several types, say $1,2,\ldots, n$, with $N = N_1 + \cdots + N_n$.
%\end{itemize}
%}
%
%\subsection{example}
%\frame{\frametitle{Example}
%Suppose you have a portfolio of $m$ independent and homogeneous risks where the total number of claims from the portfolio has a Poisson distribution with mean parameter $\lambda$.
%
%\smallskip
%Suppose the number of homogeneous risks in the portfolio was changed to $m^*$.
%
%\smallskip
%Prove that the total number of claims in the new portfolio still has a Poisson distribution. Identify its mean parameter.
%}

\section{Negative binomial random variable}
\frame{\frametitle{Negative binomial random variable}
\begin{itemize}
\item $N$ has a \alert{Negative Binomial} distribution, written $N \sim \text{NB}(\beta,r)$, if its pmf can be expressed as
\begin{equation*}
p_k = \text{Pr}(N=k) = \binom{k+r-1}{k} \biggl(\frac{1}{1+\beta} \biggr)^r \biggl(\frac{\beta}{1+\beta} \biggr)^k,
\end{equation*}
for $k=0,1,2,\ldots$ where $r>0,\beta>0$.
\smallskip
%\item Probability generating function of a Negative Binomial:
%\begin{equation*}
%P_N(z) = [1-\beta(z-1)]^{-r}.
%\end{equation*}
\item Mean: $\text{E}(N) = r \beta$
\smallskip
\item Variance: $\text{Var}(N) = r \beta (1+\beta)$.
\smallskip
\item Clearly, since $\beta>0$, the variance of the NB exceeds the mean.
\end{itemize}
}

\subsection{Geometric}
\frame{\frametitle{Geometric random variable}
\begin{itemize}
\item The \alert{Geometric} distribution is a special case of the Negative Binomial with $r=1$.
\smallskip
\item $N$ is said to be a \alert{Geometric} r.v. and written as $N \sim \text{Geometric}(p)$ if its pmf is therefore expressed as
\begin{equation*}
p_k = \text{Pr}(N=k) = \frac{1}{1+\beta} \biggl(\frac{\beta}{1+\beta} \biggr)^k, \ \ \text{for } k=0,1,2,\ldots.
\end{equation*}
\item Mean is $\text{E}(N) = \beta$ and variance is $\text{Var}(N) = \beta(1+\beta)$.
\smallskip
%\item Its pgf is:
%\begin{equation*}
%P_N(z) = \frac{1}{1-\beta(z-1)}
%\end{equation*}
\end{itemize}
}

%\subsection{mixture of Poissons}
%\frame{\frametitle{Negative Binomial as a mixture of Poissons}
%\begin{itemize}
%\item Suppose that conditionally on the parameter risk parameter $\Lambda = \lambda$, the random variable $N$ is Poisson with mean $\lambda$.
%\smallskip
%\item To evaluate the unconditional probability of $N$, use the law of total probability:
%\begin{equation*}
%p_k = \int_0^{\infty} \text{Pr}(N=k \mid \Lambda=\lambda) u(\lambda) d\lambda = \int_0^{\infty} \frac{e^{-\lambda} \lambda^k}{k!} u(\lambda) d\lambda,
%\end{equation*}
%where $u(\cdot)$ is the pdf of $\Lambda$.
%\smallskip
%\item In the case where $\Lambda$ has a gamma distribution, it can be shown (to be done in lecture) that $N$ has a Negative Binomial distribution.
%\smallskip
%\item In effect, the mixed Poisson, with a gamma mixing distribution, is equivalent to a Negative Binomial.
%\end{itemize}
%}

%\subsection{limiting case}
%\frame{\frametitle{Limiting case of the Negative Binomial}
%\begin{itemize}
%\item The Poisson distribution is a limiting case of the Negative Binomial distribution.
%\vfill \vfill \vfill
%%\smallskip
%%\item Proof to be discussed in class.
%\end{itemize}
%}
%
%\subsection{SOA question}
%\frame{\frametitle{SOA question}
%Actuaries have modeled auto windshield claim frequencies and have concluded that the number of windshield claims filed per year per driver follows the Poisson distribution with parameter $\lambda$, where $\lambda$ follows the gamma distribution with mean 3 and variance 3. \\
%\smallskip
%Calculate the probability that a driver selected at random will file no more than 1 windshield claim next year.
%}

\section{Special class of distributions}
\subsection{the $(a,b,0)$ class}
\frame{\frametitle{Special class of distributions}
\begin{itemize}
\item The $(a,b,0)$ class of distributions satisfies the recursion equations of the general form:
\begin{equation*}
\frac{p_k}{p_{k-1}} = a + \frac{b}{k}, \ \ \text{for} \ \ k=1,2,\ldots.
\end{equation*}
\item The three distributions (including Geometric as special case of NB) are the only distributions that belong to this class: Binomial, Poisson, and Negative Binomial.
\smallskip
\item It can be shown that the applicable parameters $a$ and $b$ are:
\smallskip
\begin{center}
\begin{tabular}{lll}
\hline Distribution &  & Values of $a$ and $b$ \\ \hline\hline
Binomial$(m,q)$ &  & $a =-\frac{q}{1-q}$, $b = (m+1)\frac{q}{1-q}$ \\
Poisson$(\lambda)$ &  & $a=0$, $b=\lambda $ \\
NB$(\beta,r)$ &  & $a=\frac{\beta}{1+\beta}$, $b=(r-1)\frac{\beta}{1+\beta}$ \\ \hline
\end{tabular}
\end{center}
\end{itemize}
}

\frame{\frametitle{Example}
Suppose $N$ is a counting distribution satisfying the recursive probabilities:
\begin{equation*}
\frac{p_{k}}{p_{k-1}} = \frac{4}{k}-\frac{1}{3},
\end{equation*}
for $k=1,2,\ldots$

\smallskip
Identify the distribution of $N$.
}

\frame{\frametitle{Solution to the Recursive Probability Example}
\begin{itemize}
    \item Given recursive probabilities:
    \begin{equation*}
        \frac{p_{k}}{p_{k-1}} = \frac{4}{k}-\frac{1}{3},
    \end{equation*}
    for \( k=1,2,\ldots \)

    \item Comparing with the general form of $(a,b,0)$ class:
    \begin{equation*}
        \frac{p_k}{p_{k-1}} = a + \frac{b}{k}
    \end{equation*}
    we have \( a = -\frac{1}{3} \) and \( b = 4 \). 

    \item Comparing with the known distributions, only the binomial distribution has a negative $a$. Set $a =  -\frac{1}{3}  = -\frac{q}{1-q}$ and $b = 4 = (m+1)\frac{q}{1-q}$

\end{itemize}
}


\frame{\frametitle{SOA question}
The distribution of accidents for 84 randomly selected policies is as follows:
\begin{center}
\begin{tabular}{ccc}
\hline
Number of Accidents &  & Number of Policies \\
\hline \hline
0 &  & 32 \\
1 &  & 26 \\
2 &  & 12 \\
3 &  & 7 \\
4 &  & 4 \\
5 &  & 2 \\
6 &  & 1 \\
\hline
\end{tabular}
\end{center}

\smallskip
Identify the frequency model that best represents these data.
}

\frame{\frametitle{Analyzing the Accident Data}
\begin{itemize}
    \item Given accident data for 84 policies:
    \begin{center}
    \begin{tabular}{ccc}
    \hline
    Number of Accidents (k) & Number of Policies & Ratio \( \frac{p_k}{p_{k-1}} \) \\
    \hline
    0 & 32 & - \\
    1 & 26 & \( \frac{26}{32} \approx 0.81 \) \\
    2 & 12 & \( \frac{12}{26} \approx 0.46 \) \\
    3 & 7 & \( \frac{7}{12} \approx 0.58 \) \\
    4 & 4 & \( \frac{4}{7} \approx 0.57 \) \\
    5 & 2 & \( \frac{2}{4} = 0.50 \) \\
    6 & 1 & \( \frac{1}{2} = 0.50 \) \\
    \hline
    \end{tabular}
    \end{center}

    \item Analyzing the ratio \( \frac{p_k}{p_{k-1}} \) for each \( k \) to identify a possible distribution from the $(a,b,0)$ class.
    

\end{itemize}
}


\section{Truncation and modification at zero}
\subsection{the $(a,b,1)$ class}
\frame{\frametitle{Truncation and modification at zero}
\begin{itemize}
\item The $(a,b,1)$ class of distributions satisfies the recursion equations of the general form:
\begin{equation*}
\frac{p_k}{p_{k-1}} = a + \frac{b}{k}, \ \ \text{for} \ \ k=2,3,\ldots.
\end{equation*}
\item Only difference with the $(a,b,0)$ class is the recursion here begins at $p_1$ instead of $p_0$. The values from $k=1$ to $k=\infty$ are the same up to a constant of proportionality. For the class to be a distribution, the remaining probability must be set for $k=0$.
\smallskip
\begin{itemize}
\item \alert{zero-truncated} distributions: the case when $p_0=0$
\smallskip
\item \alert{zero-modified} distributions: the case when $p_0>0$
\end{itemize}
\smallskip
\item The distributions in the second subclass is indeed a mixture of an $(a,b,0)$ and a degenerate distribution. A zero-modified distribution can be viewed as a zero-truncated by setting $p_0=0$.
\end{itemize}
}

%\subsection{the $(a,b,1)$ class}
%\frame{\frametitle{Their probability generating functions}
%\begin{itemize}
%\item We derive in class the pgf's of zero-truncated and zero-modified subclass of distributions.
%\smallskip
%\item We also discuss: ``extended'' truncated Negative Binomial (ETNB).
%\smallskip
%\item Consider also:
%\begin{itemize}
%\item Example 6.8
%\smallskip
%\item Example 6.9
%\end{itemize}
%\smallskip
%\item Check out Table 6.4, page 125 for summary of members of the $(a,b,1)$ class.
%\end{itemize}
%}

\section{Expectation and Variance for Zero-Modified and Zero-Truncated Distributions}

\frame{\frametitle{Expectation and Variance}
\framesubtitle{Zero-Modified and Zero-Truncated Distributions}
\begin{itemize}
    \item For a \alert{zero-truncated} distribution, the expected value (mean) and variance are given by:
    \begin{align*}
        \text{E}[X_{\text{trunc}}] &= \frac{\text{E}[X]}{1 - p_0}, \\
        \text{Var}[X_{\text{trunc}}] &= \frac{\text{Var}[X]}{1 - p_0} - \frac{p_0 \cdot [\text{E}[X]]^2}{(1 -p_0)^2},
    \end{align*}
    where $X_{\text{trunc}}$ is the zero-truncated version of the random variable $X$.
    
    \item For a \alert{zero-modified} distribution, assuming that $X_{\text{mod}}$ is the modified variable and $p_0$ is the modified probability at zero:
    \begin{align*}
        \text{E}[X_{\text{mod}}] &= (1 - p_0) \cdot \text{E}[X_{\text{trunc}}], \\
        \text{Var}[X_{\text{mod}}] &= (1 - p_0) \cdot \text{Var}[X_{\text{trunc}}] + p_0 \cdot (1 - p_0) \cdot [\text{E}[X_{\text{trunc}}]]^2.
    \end{align*}
\end{itemize}
}

\frame{\frametitle{Zero-Modified and Zero-Truncated Distributions}
\begin{itemize}
    \item \alert{Zero-Modified Distributions:}
    \begin{itemize}
        \item In zero-modified distributions, the probability at zero, \( p_0 \), is artificially altered.
        \item This modification changes the probabilities \( p_k \) for \( k \geq 1 \).
        \item The adjusted probabilities for \( k \geq 1 \) are:
        \begin{equation*}
            p_k' = \frac{(1 - p_0') \cdot p_k}{1 - p_0} \quad \text{for} \ k \geq 1,
        \end{equation*}
        where \( p_k \) are the original probabilities, \( p_0 \) is the original probability at zero, and \( p_0' \) is the modified probability at zero.
    \end{itemize}

    \item \alert{Zero-Truncated Distributions:}
    \begin{itemize}
        \item In a zero-truncated distribution, occurrences at zero are removed (i.e., \( p_0' = 0 \)).
        \item The probabilities \( p_k \) for \( k \geq 1 \) are scaled up so that the distribution sums to 1.
        \item The adjusted probabilities are:
        \begin{equation*}
            p_k' = \frac{p_k}{1 - p_0} \quad \text{for} \ k \geq 1,
        \end{equation*}
    \end{itemize}
\end{itemize}
}



\subsection{illustrative example}
\frame{\frametitle{Illustrative example}
Consider the zero-modified Geometric distribution with probabilities
\begin{eqnarray*}
p_0 &=& \frac{1}{2} \\
p_k &=& \frac{1}{6} \biggl( \frac{2}{3} \biggr)^{k-1}, \ \ \text{for} \ \ k=1,2,3,\ldots
\end{eqnarray*}
Derive the mean and the variance of this distribution.
}
%
%\subsection{Calculation for Truncated Geometric Distribution}
%\frame{\frametitle{Truncated Geometric Distribution}
%\begin{itemize}
%    \item Starting with a Geometric distribution parameterized by \( \beta = \frac{1}{2} \), the mean and variance are:
%    \begin{align*}
%        \text{E}[N] &= \frac{1}{2}, \\
%        \text{Var}[N] &= \frac{3}{4}.
%    \end{align*}
%
%    \item For the zero-truncated version, the mean and variance are adjusted as follows:
%    \begin{align*}
%        \text{E}[N_{\text{trunc}}] &= \frac{\text{E}[N]}{1 - p_0} = \frac{\frac{1}{2}}{1 - \frac{1}{2}} = 1, \\
%        \text{Var}[N_{\text{trunc}}] &= \frac{\text{Var}[N]}{1 - p_0} - \frac{p_0 \cdot [\text{E}[N]]^2}{(1 - p_0)^2} \\
%        &= \frac{\frac{3}{4}}{1 - \frac{1}{2}} - \frac{\frac{1}{2} \cdot \left(\frac{1}{2}\right)^2}{(1 - \frac{1}{2})^2} = \frac{3}{2} - \frac{1}{4} = \frac{5}{4}.
%    \end{align*}
%
%\end{itemize}
%}
%
%\subsection{Solution to the Illustrative Example}
%\frame{\frametitle{Zero-Modified Geometric Distribution}
%\begin{itemize}
%    \item Applying the zero-modification formulas to the truncated values:
%    \begin{align*}
%        \text{E}[N_{\text{mod}}] &= (1 - p_0) \cdot \text{E}[N_{\text{trunc}}] = \frac{1}{2} \cdot 1 = \frac{1}{2}, \\
%        \text{Var}[N_{\text{mod}}] &= (1 - p_0) \cdot \text{Var}[N_{\text{trunc}}] + p_0 \cdot (1 - p_0) \cdot [\text{E}[N_{\text{trunc}}]]^2 \\
%        &= \frac{1}{2} \cdot \frac{5}{4} + \frac{1}{2} \cdot \frac{1}{2} \cdot 1^2 \\
%        &= \frac{5}{8} + \frac{1}{4} \\
%        &= \frac{7}{8}.
%    \end{align*}
%
%    \item Therefore, the mean and variance of the zero-modified Geometric distribution are \( \frac{1}{2} \) and \( \frac{7}{8} \), respectively.
%\end{itemize}
%}
%

\end{document}
